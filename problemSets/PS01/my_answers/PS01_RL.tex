\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx,textcomp}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{color}
\usepackage[reqno]{amsmath}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{amssymb,enumerate}
\usepackage[all]{xy}
\usepackage{endnotes}
\usepackage{lscape}
\newtheorem{com}{Comment}
\usepackage{float}
\usepackage{hyperref}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
\usepackage[compact]{titlesec}
\usepackage{dcolumn}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{xcolor}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\definecolor{light-gray}{gray}{0.65}
\usepackage{url}
\usepackage{listings}
\usepackage{color}
\usepackage{relsize}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}
\newcommand{\Sref}[1]{Section~\ref{#1}}
\newtheorem{hyp}{Hypothesis}

\title{Problem Set 1}
\date{Due: February 11, 2026}
\author{Applied Stats II}


\begin{document}
	\maketitle
	\section*{Instructions}
	\begin{itemize}
	\item Please show your work! You may lose points by simply writing in the answer. If the problem requires you to execute commands in \texttt{R}, please include the code you used to get your answers. Please also include the \texttt{.R} file that contains your code. If you are not sure if work needs to be shown for a particular problem, please ask.
\item Your homework should be submitted electronically on GitHub in \texttt{.pdf} form.
\item This problem set is due before 23:59 on Wednesday February 11, 2026. No late assignments will be accepted.
	\end{itemize}

	\vspace{.25cm}
\section*{Question 1} 
\vspace{.25cm}
\noindent The Kolmogorov-Smirnov test uses cumulative distribution statistics test the similarity of the empirical distribution of some observed data and a specified PDF, and serves as a goodness of fit test. The test statistic is created by:

$$D = \max_{i=1:n} \Big\{ \frac{i}{n}  - F_{(i)}, F_{(i)} - \frac{i-1}{n} \Big\}$$

\noindent where $F$ is the theoretical cumulative distribution of the distribution being tested and $F_{(i)}$ is the $i$th ordered value. Intuitively, the statistic takes the largest absolute difference between the two distribution functions across all $x$ values. Large values indicate dissimilarity and the rejection of the hypothesis that the empirical distribution matches the queried theoretical distribution. The p-value is calculated from the Kolmogorov-
Smirnoff CDF:

$$p(D \leq d)= \frac{\sqrt {2\pi}}{d} \sum _{k=1}^{\infty }e^{-(2k-1)^{2}\pi ^{2}/(8d^{2})}$$


\noindent which generally requires approximation methods (see \href{https://core.ac.uk/download/pdf/25787785.pdf}{Marsaglia, Tsang, and Wang 2003}). This so-called non-parametric test (this label comes from the fact that the distribution of the test statistic does not depend on the distribution of the data being tested) performs poorly in small samples, but works well in a simulation environment. Write an \texttt{R} function that implements this test where the reference distribution is normal. Using \texttt{R} generate 1,000 Cauchy random variables (\texttt{rcauchy(1000, location = 0, scale = 1)}) and perform the test (remember, use the same seed, something like \texttt{set.seed(123)}, whenever you're generating your own data).\\


\noindent\textbf{My Answer:}
\vspace{.25cm}

\noindent Utilizing the Kolmogorov-Smirnoff test, 1,000 Cauchy random variables will be tested against 1,000 normally distributed random variables. The null hypothesis states that our empirical data (Cauchy data) come from a normal distribution and that the maximum absolute difference between distributions is equal to 0. The alternative hypothesis states that the empirical data do not come from a normal distribution and that the maximum absolute difference between the two distributions is greater than 0.

\begin{center}
\textbf{$H_{0}$} : D - $d_{obs}$ = 0, \textbf{$H_{a}$} : D - $d_{obs}$ $>$ 0.
\end{center}

\noindent In order to test my hypotheses, I created a function in R that determines the test statistic and p-value for comparing an empirical set of data to a normal reference distribution. As the empirical CDF is a step-wise function, the calculation for the test statistic is most accurately completed when considering the gap between the two distributions at each step and just before the step occurs. The value d\_plus is the maximum distance at each step, which is how much the ECDF exceeds the theoretical CDF. The value d\_minus is the maximum distance between the theoretical CDF and ECDF just prior to the step. 1/length(data) is taken from each ECDF data point prior to each step, as that is the amount that the ECDF increases at each step. I chose to utilize a line of code cited in Marsaglia, Tang, and Wang (2003), that efficiently determines the p-value given a sample larger than 99, as well as a value of D*D*n that is larger than 3.76, both of which apply to the data we have.
\vspace{.25cm}

	\lstinputlisting[language=R, firstline=39, lastline=70]{PS01_RL.R}
	\vspace{.25cm}
	
\noindent Implementing my function on 1,000 Cauchy random variables to test my hypothesis, and comparing this to the built-in ks.test() function in R to ensure my function is accurate.
\vspace{.25cm}

	\lstinputlisting[language=R, firstline=72, lastline=80]{PS01_RL.R}
	\vspace{.25cm}

\textbf{ks\_test(data):} P-value = 1.60e-16; D = 0.136
\vspace{.25cm}

\textbf{ks.test(data):} P-value = 2.22e-16; D = 0.136
\vspace{.25cm}

\noindent These values confirm that my function accurately completes the Kolmogorov-Smirnoff test. There is only a small variation in the p-value generated using my function and the ks.test() function, that could be attributed to approximations made. The small p-value also allows for the conclusion that the null hypothesis can be rejected. This means that the empirical data do not come from a normal distribution and the maximum absolute difference between the two distributions is significantly greater than 0. This can be confirmed by the fact that the Cauchy distribution is not a normal distribution.

\newpage

\section*{Question 2}
\noindent Estimate an OLS regression in \texttt{R} that uses the Newton-Raphson algorithm (specifically \texttt{BFGS}, which is a quasi-Newton method), and show that you get the equivalent results to using \texttt{lm}. Use the code below to create your data.
\vspace{.5cm}
\lstinputlisting[language=R, firstline=86,lastline=93]{PS01_RL.R} 
\vspace{.25cm}

\noindent\textbf{My Answer:}
\vspace{.25cm}

\noindent In order to estimate an OLS regression in \texttt{R} that uses the Newton-Raphson algorithm, an OLS likelihood function needs to be established. This likelihood function will measure the likelihood of observing the outcome at given parameter values (beta and sigma), allowing for the maximum likelihood to be found, and subsequently the optimal parameter values. In order to create this function in R, the n is established as the number of columns in the input, which are the number of columns in the model matrix, equivalent to the number of parameters to be determined. The beta object will then take the 1:n values in the parameter and use them as the starting parameter values. The sigma is established as the exponentiated value of the final parameter, which is the error. In order to not introduce NA values due to taking the square root of the final parameter sigma, exponentiation was used. The log-likelihood values for the normal density distribution are determined using the dnorm function, taking the outcome, as well as the predicted means of each observation, calculated by using matrix multiplication on the input and beta matrices. Sigma is the standard deviation of the distribution. A negative is used prior to the summation of the likelihood values, as the optim() function takes the minimum of the likelihood function to find the optimal parameter.
\vspace{.25cm}

\lstinputlisting[language=R, firstline=95,lastline=103]{PS01_RL.R} 
\vspace{.25cm}

\noindent The optim() function is subsequently utilized, implementing the ols\_likelihood function, with the outcome being the y-values of the data, and the input being a matrix consisting of a column of 1's and a column of the x-values of the data. The initial values of the parameters are set at 1, hessian is set to TRUE to return a hessian matrix, and the method used is BFGS, which is the Newton-Raphson method.
\vspace{.25cm}

\lstinputlisting[language=R, firstline=105,lastline=111]{PS01_RL.R} 
\vspace{.25cm}

\noindent Finally, the parameter results using MLE and the Newton-Raphson method can be compared to the results using the lm() function.
\vspace{.25cm}

\lstinputlisting[language=R, firstline=113,lastline=118]{PS01_RL.R} 

\begin{verbatim}
	--- results_norm$par ---
	[1] 0.1392 2.7267

	--- lm Results ---
		Coefficients:
	(Intercept)       data$x  
	   0.1392          2.7267  
\end{verbatim}

\noindent As seen above, using the lm() function and estimating an OLS regression using maximum likelihood and the Newton-Raphson algorithm result in the same parameter values. This is due to the fact that under a normal distribution, and specifcally with normally distributed error terms, finding the maximum log-likelihood estimates for the parameters reduces down to the same mathematical formula as determining the parameters based on minimizing the least squares estimates.

\end{document}
